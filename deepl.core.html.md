# Fundamental functions for time series modeling using deep learning
methods in pytorch


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

``` python
from pathlib import Path


DATA_PATH = Path("../testing_data")
```

## Data Preprocessing

We will first open the data

``` python
data =pd.read_csv(DATA_PATH / "hydro_example.csv", parse_dates=True, index_col="time")
data.head(5)
```

Now we will split data into coherent groups

------------------------------------------------------------------------

<a
href="https://github.com/iraind/omvs_senegal/blob/main/omvs_senegal/deepl/core.py#L16"
target="_blank" style="float:right; font-size:smaller">source</a>

### split_by_date

>  split_by_date (data:pandas.core.frame.DataFrame, val_dates:tuple,
>                     test_dates:tuple)

*Split time series data into train, validation and test sets based on
date ranges.*

<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 38%" />
<col style="width: 52%" />
</colgroup>
<thead>
<tr>
<th></th>
<th><strong>Type</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>data</td>
<td>DataFrame</td>
<td>Input dataframe containing time series data</td>
</tr>
<tr>
<td>val_dates</td>
<td>tuple</td>
<td>Tuple of (start_date, end_date) for validation set</td>
</tr>
<tr>
<td>test_dates</td>
<td>tuple</td>
<td>Tuple of (start_date, end_date) for test set</td>
</tr>
<tr>
<td><strong>Returns</strong></td>
<td><strong>tuple</strong></td>
<td></td>
</tr>
</tbody>
</table>

``` python
train, valid, test = split_by_date(data, val_dates=("2012-01-01", "2012-12-31"), test_dates=("2013-01-01", "2014-12-31"))
```

Now lets define the feature and the target columns and divide data in
feature and targets

``` python
x_cols = ["smoothed_rain","Q_mgb"]
y_cols = ["Q_obs"]

x_train, y_train = train[x_cols], train[y_cols]
x_valid, y_valid = valid[x_cols], valid[y_cols]
x_test, y_test = test[x_cols], test[y_cols]
```

Now we will fit the scaler based only on train data. This ensures
that: 1. No information from the validation/test data sets leaks to into
the scaling process 2. All data is scaled consistently using the same
parameters 3. The model sees new data scaled in the same way as it was
trained

``` python
feature_scaler, target_scaler = RobustScaler(), RobustScaler()
_, _ = feature_scaler.fit_transform(x_train), target_scaler.fit_transform(y_train)
```

Finally, weâ€™ll create a custom dataset class to handle our time series
data. This class will create sequences of input features (simulation
discharge and rainfall) and target values (observed discharge).

------------------------------------------------------------------------

<a
href="https://github.com/iraind/omvs_senegal/blob/main/omvs_senegal/deepl/core.py#L32"
target="_blank" style="float:right; font-size:smaller">source</a>

### HydroDataset

>  HydroDataset (x:pandas.core.frame.DataFrame,
>                    y:pandas.core.frame.DataFrame, ctx_len:int,
>                    pred_len:int=10, x_transform:<built-
>                    infunctioncallable>=None, y_transform:<built-
>                    infunctioncallable>=None)

\*An abstract class representing a :class:`Dataset`.

All datasets that represent a map from keys to data samples should
subclass it. All subclasses should overwrite :meth:`__getitem__`,
supporting fetching a data sample for a given key. Subclasses could also
optionally overwrite :meth:`__len__`, which is expected to return the
size of the dataset by many :class:`~torch.utils.data.Sampler`
implementations and the default options of
:class:`~torch.utils.data.DataLoader`. Subclasses could also optionally
implement :meth:`__getitems__`, for speedup batched samples loading.
This method accepts list of indices of samples of batch and returns list
of samples.

.. note:: :class:`~torch.utils.data.DataLoader` by default constructs an
index sampler that yields integral indices. To make it work with a
map-style dataset with non-integral indices/keys, a custom sampler must
be provided.\*

We can easily instantiate the dataset as follows

``` python
train_dataset = HydroDataset(
    x=x_train,
    y=y_train,
    ctx_len=1,
    pred_len=1,
    x_transform=feature_scaler.transform,
    y_transform=target_scaler.transform
    )
```

The total training samples are

``` python
len(train_dataset)
```

Is it possible to easly get a training sample as follows:

``` python
train_dataset[5]
```

And also to the the t+0 for any item

``` python
train_dataset.get_t0(1000)
```

## Model example

For the sake of example, we will define the simplest NN we possibly can
in PyTorch, which is a simple linear model.

``` python
class SimpleNN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(SimpleNN, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        batch_size = x.shape[0]
        x = x.reshape(batch_size, -1)
        out = self.linear(x)
        return out
```

## Model training

Now we will define a basic learner class to handle the training process.
This class will be used to train the model and evaluate its performance.

------------------------------------------------------------------------

<a
href="https://github.com/iraind/omvs_senegal/blob/main/omvs_senegal/deepl/core.py#L71"
target="_blank" style="float:right; font-size:smaller">source</a>

### Learner

>  Learner (model:torch.nn.modules.module.Module,
>               train_loader:torch.utils.data.dataloader.DataLoader,
>               val_loader:torch.utils.data.dataloader.DataLoader,
>               criterion:torch.nn.modules.module.Module=MSELoss(),
>               optimizer:torch.optim.optimizer.Optimizer=<class
>               'torch.optim.adam.Adam'>, log_dir:str=None, verbose:bool=True)

*Initialize self. See help(type(self)) for accurate signature.*

<table>
<thead>
<tr>
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>Module</td>
<td></td>
<td>model to train</td>
</tr>
<tr>
<td>train_loader</td>
<td>DataLoader</td>
<td></td>
<td>data loader for training data</td>
</tr>
<tr>
<td>val_loader</td>
<td>DataLoader</td>
<td></td>
<td>data loader for validation data</td>
</tr>
<tr>
<td>criterion</td>
<td>Module</td>
<td>MSELoss()</td>
<td>loss function to optimize</td>
</tr>
<tr>
<td>optimizer</td>
<td>Optimizer</td>
<td>Adam</td>
<td>optimizer class to use for training</td>
</tr>
<tr>
<td>log_dir</td>
<td>str</td>
<td>None</td>
<td>directory to save tensorboard logs,</td>
</tr>
<tr>
<td>verbose</td>
<td>bool</td>
<td>True</td>
<td>whether to print training progress</td>
</tr>
<tr>
<td><strong>Returns</strong></td>
<td><strong>None</strong></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

## Model training example

Lets see a simple example of how we can train a neural network.

First we will create our Datasets and Dataloarders based on the data we
splitted above

``` python
batch_size = 32

context_len=3
prediction_len=2
x_transform=feature_scaler.transform
y_transform=target_scaler.transform

train_dataset = HydroDataset(x=x_train, y=y_train, ctx_len=context_len, pred_len=prediction_len, x_transform=x_transform, y_transform=y_transform)
valid_dataset = HydroDataset(x=x_valid, y=y_valid, ctx_len=context_len, pred_len=prediction_len, x_transform=x_transform, y_transform=y_transform)
test_dataset = HydroDataset(x=x_test, y=y_test, ctx_len=context_len, pred_len=prediction_len, x_transform=x_transform, y_transform=y_transform)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)
```

We can now instantiate the model

``` python
model = SimpleNN(input_dim=len(x_cols)*context_len, output_dim=prediction_len)
```

And finally we can instantiate the learner and fit our data

``` python
learner = Learner(model=model, train_loader=train_loader, val_loader=valid_loader)
learner.fit(lr=0.001, epochs=3)
```

Lets now see the prediction. There are two possible ways. Predicting
only the values.

``` python
y_pred = learner.predict_values(test_loader)
```

Getting the prediction with the timestamp and column name. This allow us
also to scale back to the original values.

``` python
y_pred = learner.predict(test_loader, inverse_transform=target_scaler.inverse_transform)
y_pred.head(4)
```

We will now add the observation and the mgb simulation so we can plot
the result.

``` python
y_pred["obs"] = y_test.loc[y_pred.index]
y_pred["mgb"] = x_test["Q_mgb"].loc[y_pred.index]
y_pred.plot()
```
