{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# L'objectif est d'expÃ©rimenter diffÃ©rentes tailles de fenÃªtres temporelles (window_size) pour trouver celle qui donne les meilleures performances.\n",
    "\n",
    "## MÃ©thodologie\n",
    "* DÃ©finir une liste de tailles de fenÃªtres (window_size) Ã  tester, par exemple [30, 60, 90, 120].\n",
    "* CrÃ©er des sÃ©quences avec chaque window_size et un prediction_size fixe.\\\\\n",
    "* EntraÃ®ner le modÃ¨le LSTM sur chaque fenÃªtre.\n",
    "* Ã‰valuer les performances avec RMSE, MAE et RÂ².\n",
    "* Comparer les performances pour choisir la meilleure fenÃªtre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Preguntas\n",
    "\n",
    "- Â¿Por quÃ©, si la validation loss oscila tanto, no paras el entrenamiento antes?\n",
    "- Â¿Que es RobustNormalization?\n",
    "- AÃ±adir tensor board para seguir el entrenamiento\n",
    "- Quizas no sÃ©a relevante para el entrenamiento y la predicciÃ³n del modelo pero Â¿el hecho de que robust scaler haga que haya lluvia negativa no va a afectar? Quizas habrÃ­a que revisarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from ombs_senegal.time_series_deepl import Learner, HydroDataset, split_by_date\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "DATA_PATH = Path(\"../../data\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    DATA_PATH/'data_cumul.csv', \n",
    "    sep=';', \n",
    "    usecols=['time', 'dÃ©bit_insitu', 'P_cumul_7j', 'dÃ©bit_mgb'], \n",
    "    index_col='time',\n",
    "    converters={\"time\": pd.to_datetime}\n",
    "    )\n",
    "data = data[\"2012-01-01\":]\n",
    "data[\"mois\"] = data.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = split_by_date(data, val_dates=(\"2018-01-01\", \"2018-12-31\"), test_dates=(\"2019-01-01\", \"2020-12-31\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Now lets define the feature and the target columns and divide data in feature and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [\"dÃ©bit_mgb\",\"P_cumul_7j\", \"mois\"]\n",
    "y_cols = [\"dÃ©bit_insitu\"]\n",
    "\n",
    "x_train, y_train = train[x_cols], train[y_cols]\n",
    "x_valid, y_valid = valid[x_cols], valid[y_cols]\n",
    "x_test, y_test = test[x_cols], test[y_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Now we will fit the scaler based only on train data. This ensures that:\n",
    "1. No information from the validation/test data sets leaks to into the scaling process\n",
    "2. All data is scaled consistently using the same parameters\n",
    "3. The model sees new data scaled in the same way as it was trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler, target_scaler = RobustScaler(), RobustScaler()\n",
    "_, _ = feature_scaler.fit_transform(x_train), target_scaler.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Multi layer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRegularizedMLP(nn.Module):\n",
    "    def __init__(self, input_size, prediction_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.norm = nn.LayerNorm(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, prediction_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRegularizedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, prediction_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_size, prediction_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        x = hn[-1]\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, prediction_size):\n",
    "        super().__init__()\n",
    "        self.bilstm1 = nn.LSTM(input_size, 256, bidirectional=True, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.bilstm2 = nn.LSTM(512, 128, bidirectional=True, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.lstm = nn.LSTM(256, 64, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.Linear(64, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.dense2 = nn.Linear(128, prediction_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.bilstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.bilstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        #x, _ = self.lstm(x)\n",
    "        x, (hn, _) = self.lstm(x)\n",
    "        x = hn[-1]   \n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Gated Recurrent Units (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRegularizedGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, prediction_size):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_size, prediction_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, hn = self.gru(x)\n",
    "        x = hn[-1]\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TemporalCausalCNN(nn.Module):\n",
    "    def __init__(self, input_channels, window_size, prediction_size):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=3)\n",
    "        self.norm1 = nn.LayerNorm([window_size, 32])\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3)\n",
    "        self.norm2 = nn.LayerNorm([window_size, 64])\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(64, prediction_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Entrada: (batch, time, variables) â†’ permutar para Conv1d\n",
    "        x = x.permute(0, 2, 1)  # (B, C, T)\n",
    "\n",
    "        # Padding causal antes de cada convoluciÃ³n\n",
    "        x = F.pad(x, (2, 0))  # padding izquierda = kernel_size - 1\n",
    "        x = self.conv1(x)     # (B, 32, T)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = F.pad(x, (2, 0))\n",
    "        x = self.conv2(x)     # (B, 64, T)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = self.global_pool(x).squeeze(-1)  # (B, 64)\n",
    "        x = self.fc(x)                       # (B, prediction_size)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from ombs_senegal.benchmark_model import BenchmarkScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_scores = BenchmarkScores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ”¹ Listes des tailles de fenÃªtres Ã  tester\n",
    "context_sizes = [15]#, 30, 60, 90]\n",
    "batch_size = 32\n",
    "learning_rate = 0.0003\n",
    "epochs=1\n",
    "\n",
    "prediction_size = 10  # Fixe (peut Ãªtre ajustÃ©)\n",
    "x_transform=feature_scaler.transform\n",
    "y_transform=target_scaler.transform\n",
    "results = []\n",
    "models = []\n",
    "\n",
    "# ðŸ”¹ Boucle sur diffÃ©rentes tailles de fenÃªtres\n",
    "for context_size in context_sizes:\n",
    "    print(f\"\\nðŸŸ¢ Test avec window_size = {context_size}\")\n",
    "\n",
    "    train_dataset = HydroDataset(x=x_train, y=y_train, ctx_len=context_size, pred_len=prediction_size, x_transform=x_transform, y_transform=y_transform)\n",
    "    valid_dataset = HydroDataset(x=x_valid, y=y_valid, ctx_len=context_size, pred_len=prediction_size, x_transform=x_transform, y_transform=y_transform)\n",
    "    test_dataset = HydroDataset(x=x_test, y=y_test, ctx_len=context_size, pred_len=prediction_size, x_transform=x_transform, y_transform=y_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # ðŸ”¹ VÃ©rification des dimensions\n",
    "    # model = LSTMModel(len(x_cols), prediction_size).to(DEVICE)\n",
    "    #model = SimpleRegularizedLSTM(len(x_cols), 64, prediction_size).to(DEVICE)\n",
    "    # model = SimpleRegularizedGRU(len(x_cols), 64, prediction_size).to(DEVICE)\n",
    "    # model = TemporalCausalCNN(len(x_cols), context_size, prediction_size).to(DEVICE)\n",
    "    model = SimpleRegularizedMLP(len(x_cols)*context_size, prediction_size).to(DEVICE)\n",
    "    learner = Learner(model=model, train_loader=train_loader, val_loader=valid_loader)\n",
    "    learner.fit(lr=learning_rate, epochs=epochs)\n",
    "\n",
    "    y_pred = learner.predict(test_loader, inverse_transform=target_scaler.inverse_transform)\n",
    "\n",
    "    y_pred.index.name = \"time\"\n",
    "    y_pred[\"model\"] = model.__class__.__name__\n",
    "    y_pred.set_index([\"model\"], append=True, inplace=True)\n",
    "    y_pred = y_pred.to_xarray()\n",
    "    scores = benchmark_scores.compute_scores(y_pred, y_test.to_xarray()[y_cols[0]], metrics=[\"mae\", \"rmse\", \"nse\", \"kge\"])\n",
    "\n",
    "    mean_scores = {s.upper(): round(float(scores[s].mean().values), 2) for s in scores.data_vars}\n",
    "    print(f\"ðŸ“Š RÃ©sultats pour window_size={context_size} -> {mean_scores}\")\n",
    "\n",
    "    # ðŸ”¹ Stocker les rÃ©sultats\n",
    "    results.append({\"ctx_size\": context_size, **mean_scores})\n",
    "\n",
    "# ðŸ”¹ Afficher le meilleur rÃ©sultat\n",
    "best = min(results, key=lambda x: x[\"RMSE\"])  # Choix basÃ© sur le RMSE le plus bas\n",
    "print(f\"\\nâœ… Meilleure fenÃªtre : {best[\"ctx_size\"]} avec RMSE={best[\"RMSE\"]}, MAE={best[\"MAE\"]}, NSE={best[\"NSE\"]}, KGE={best[\"KGE\"]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "- MLP: Meilleure fenÃªtre : 60 avec RMSE=156.864, MAE=78.964, RÂ²=0.861\n",
    "- CNN: Meilleure fenÃªtre : 30 avec RMSE=191.998, MAE=92.670, RÂ²=0.786\n",
    "- GRU: Meilleure fenÃªtre : 60 avec RMSE=212.033, MAE=108.959, RÂ²=0.746\n",
    "- Simple LSTM: Meilleure fenÃªtre : 90 avec RMSE=197.514, MAE=104.173, RÂ²=0.785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ Fonction pour calculer le PBIAS\n",
    "def pbias(y_true, y_pred):\n",
    "    return 100 * np.sum(y_pred - y_true) / np.sum(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Learning rate finder development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_find(self, start_lr=1e-7, end_lr=10, num_iter=100, step_mode=\"exp\", show_plot=True):\n",
    "        \"\"\"Find a good learning rate by training with exponentially growing lr\n",
    "            source: https://github.com/fastai/fastai1/blob/master/fastai/train.py#L33\n",
    "\n",
    "        \n",
    "        Args:\n",
    "            start_lr (float): Starting learning rate\n",
    "            end_lr (float): Maximum learning rate\n",
    "            num_iter (int): Number of iterations to run\n",
    "            step_mode (str): \"exp\" for exponential increase, \"linear\" for linear increase\n",
    "            show_plot (bool): Whether to display the loss plot\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (optimal_lr, learning_rates, losses)\n",
    "        \"\"\"\n",
    "        # Save the original model state\n",
    "        original_state = {\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer\n",
    "        }\n",
    "        \n",
    "        # Initialize optimizer with start_lr\n",
    "        optimizer = self.optimizer(self.model.parameters(), lr=start_lr)\n",
    "        \n",
    "        # Calculate the multiplication factor for each step\n",
    "        if step_mode == \"exp\":\n",
    "            gamma = (end_lr / start_lr) ** (1 / num_iter)\n",
    "        else:\n",
    "            gamma = (end_lr - start_lr) / num_iter\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma) if step_mode == \"exp\" else None\n",
    "        \n",
    "        learning_rates = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        # Create iterator for training data\n",
    "        iterator = iter(self.train_loader)\n",
    "        \n",
    "        for iteration in range(num_iter):\n",
    "            try:\n",
    "                batch_X, batch_y = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(self.train_loader)\n",
    "                batch_X, batch_y = next(iterator)\n",
    "                \n",
    "            # Forward pass\n",
    "            self.model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(batch_X)\n",
    "            loss = self.criterion(outputs, batch_y.squeeze())\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Store the values\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            learning_rates.append(current_lr)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Update learning rate\n",
    "            if step_mode == \"exp\":\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = start_lr + (gamma * (iteration + 1))\n",
    "            \n",
    "            # Stop if the loss is exploding\n",
    "            if iteration > 0 and losses[-1] > 4 * best_loss:\n",
    "                break\n",
    "                \n",
    "            if losses[-1] < best_loss:\n",
    "                best_loss = losses[-1]\n",
    "        \n",
    "        # Restore the original model state\n",
    "        self.model.load_state_dict(original_state['model'])\n",
    "        \n",
    "        if show_plot:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(learning_rates, losses)\n",
    "            plt.xscale('log')\n",
    "            plt.xlabel('Learning Rate (log scale)')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Learning Rate Finder')\n",
    "            plt.show()\n",
    "            \n",
    "        # Find the point of steepest descent\n",
    "        smoothed_losses = np.array(losses)\n",
    "        min_grad_idx = np.gradient(smoothed_losses).argmin()\n",
    "        optimal_lr = learning_rates[min_grad_idx]\n",
    "            \n",
    "        return optimal_lr, learning_rates, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
