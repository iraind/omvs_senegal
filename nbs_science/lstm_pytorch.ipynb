{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# L'objectif est d'exp√©rimenter diff√©rentes tailles de fen√™tres temporelles (window_size) pour trouver celle qui donne les meilleures performances.\n",
    "\n",
    "## M√©thodologie\n",
    "* D√©finir une liste de tailles de fen√™tres (window_size) √† tester, par exemple [30, 60, 90, 120].\n",
    "* Cr√©er des s√©quences avec chaque window_size et un prediction_size fixe.\\\\\n",
    "* Entra√Æner le mod√®le LSTM sur chaque fen√™tre.\n",
    "* √âvaluer les performances avec RMSE, MAE et R¬≤.\n",
    "* Comparer les performances pour choisir la meilleure fen√™tre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Preguntas\n",
    "\n",
    "- ¬øPor qu√©, si la validation loss oscila tanto, no paras el entrenamiento antes?\n",
    "- ¬øQue es RobustNormalization?\n",
    "- A√±adir tensor board para seguir el entrenamiento\n",
    "- Quizas no s√©a relevante para el entrenamiento y la predicci√≥n del modelo pero ¬øel hecho de que robust scaler haga que haya lluvia negativa no va a afectar? Quizas habr√≠a que revisarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q plotly tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "DATA_PATH = Path(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_PATH / \"data_cumul.csv\", delimiter=\";\", parse_dates=True, index_col=\"time\")\n",
    "data = data[[\"P_cumul_7j\",\"d√©bit_mgb\",\"d√©bit_insitu\"]]\n",
    "data = data[\"2012-01-01\":]\n",
    "data[\"mois\"] = data.index.month\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(features_scaled, index=data.index, columns=['P_cumul_7j','d√©bit_mgb',\"mois\"])[\"P_cumul_7j\"].plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "# Mise √† l'√©chelle avec RobustScaler\n",
    "scaler_features = RobustScaler()\n",
    "scaler_target = RobustScaler()\n",
    "#features =data[[\"d√©bit_mgb_imerg\",\"Q1_IMERG\",\"Q2_IMERG\",\"Q3_IMERG\",\"mgb_wse_imerg\"]].values\n",
    "features =data[['P_cumul_7j','d√©bit_mgb',\"mois\"]].values\n",
    "#features=data[[\"d√©bit_mgb_imerg\",\"Q1_IMERG\",\"Q2_IMERG\",\"Q3_IMERG\",\"mgb_wse_imerg\"]].values\n",
    "target =data['d√©bit_insitu'].values.reshape(-1, 1)\n",
    "targets =data['d√©bit_mgb'].values.reshape(-1, 1)\n",
    "features_scaled = scaler_features.fit_transform(features)\n",
    "target_scaled = scaler_target.fit_transform(target)\n",
    "targets_scaled = scaler_target.fit_transform(targets)\n",
    "\n",
    "# Diviser les donn√©es en train, validation et test\n",
    "train_size = int(len(features_scaled) * 0.6)\n",
    "val_size = int(len(features_scaled) * 0.2)\n",
    "test_size = len(features_scaled) - train_size - val_size\n",
    "\n",
    "train_features = features_scaled[:train_size]\n",
    "train_target = target_scaled[:train_size]\n",
    "\n",
    "val_features = features_scaled[train_size:train_size + val_size]\n",
    "val_target = target_scaled[train_size:train_size + val_size]\n",
    "\n",
    "test_features = features_scaled[train_size + val_size:]\n",
    "test_target = target_scaled[train_size + val_size:]\n",
    "test_targets = targets_scaled[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, window_size, prediction_size):\n",
    "        super().__init__()\n",
    "        self.bilstm1 = nn.LSTM(input_size, 256, bidirectional=True, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.bilstm2 = nn.LSTM(512, 128, bidirectional=True, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.lstm = nn.LSTM(256, 64, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.Linear(64, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.dense2 = nn.Linear(128, prediction_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.bilstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.bilstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # Take last output\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    # üîπ Initialisation du mod√®le\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Learner definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Learner:\n",
    "    def __init__(self,\n",
    "                 model: nn.Module, # model to train\n",
    "                 train_loader: DataLoader, # data loader for training data\n",
    "                 val_loader: DataLoader, # data loader for validation data\n",
    "                 criterion: nn.Module = nn.MSELoss(), # loss function to optimize\n",
    "                 optimizer: torch.optim.Optimizer = torch.optim.Adam, # optimizer class to use for training\n",
    "                 log_dir: str = 'runs', # directory to save tensorboard logs\n",
    "                 ) -> None:\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "\n",
    "    def fit(self, lr=0.001, epochs=10):\n",
    "        optimizer = self.optimizer(self.model.parameters(), lr=lr)\n",
    "        for epoch in tqdm(range(epochs), desc='Training epochs'):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            for batch_X, batch_y in self.train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = epoch_loss/len(self.train_loader)\n",
    "            self.writer.add_scalar('Training Loss/epoch', avg_train_loss, epoch)\n",
    "\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in self.val_loader:\n",
    "                    val_outputs = self.model(batch_X)\n",
    "                    val_loss += self.criterion(val_outputs, batch_y.squeeze()).item()\n",
    "            \n",
    "            avg_val_loss = val_loss/len(self.val_loader)\n",
    "            self.writer.add_scalar('Validation Loss/epoch', avg_val_loss, epoch)\n",
    "            \n",
    "            # print(f'Epoch {epoch+1}, Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    def predict(self, dl: DataLoader):\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in dl:\n",
    "                batch_pred = self.model(batch_X).cpu().numpy()\n",
    "                predictions.append(batch_pred)\n",
    "        \n",
    "        return np.vstack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(X_train.shape[2], window_size, prediction_size).to(device)\n",
    "learner = Learner(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# üîπ D√©finition de la fonction pour cr√©er des s√©quences\n",
    "def create_sequences(features, target, window_size, prediction_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - window_size - prediction_size):\n",
    "        X.append(features[i:i+window_size])\n",
    "        y.append(target[i+window_size:i+window_size+prediction_size])\n",
    "    return torch.FloatTensor(X).to(device), torch.FloatTensor(y).to(device)\n",
    "\n",
    "# üîπ Listes des tailles de fen√™tres √† tester\n",
    "window_sizes = [10,]#20,30, 60, 90, 120]\n",
    "prediction_size = 10  # Fixe (peut √™tre ajust√©)\n",
    "batch_size = 48  # D√©finition de la taille des batchs\n",
    "results = []\n",
    "models = []\n",
    "\n",
    "# üîπ Boucle sur diff√©rentes tailles de fen√™tres\n",
    "for window_size in window_sizes:\n",
    "    print(f\"\\nüü¢ Test avec window_size = {window_size}\")\n",
    "\n",
    "    # Cr√©ation des s√©quences\n",
    "    X_train, y_train = create_sequences(train_features, train_target, window_size, prediction_size)\n",
    "    X_val, y_val = create_sequences(val_features, val_target, window_size, prediction_size)\n",
    "    X_test, y_test = create_sequences(test_features, test_target, window_size, prediction_size)\n",
    "\n",
    "    # Cr√©ation des DataLoaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
    "    break\n",
    "\n",
    "    # üîπ V√©rification des dimensions\n",
    "    model = LSTMModel(X_train.shape[2], window_size, prediction_size).to(device)\n",
    "    learner = Learner(model, train_loader, val_loader)\n",
    "    learner = Learner(model, train_loader, val_loader)\n",
    "    learner.fit(lr=0.0001, epochs=20)\n",
    "\n",
    "    y_pred = learner.predict(test_loader)\n",
    "\n",
    "    # üîπ Inversion de l'√©chelle si n√©cessaire\n",
    "    y_test_rescaled = scaler_target.inverse_transform(y_test.reshape(-1, prediction_size))\n",
    "    y_pred_rescaled = scaler_target.inverse_transform(y_pred.reshape(-1, prediction_size))\n",
    "\n",
    "    # üîπ Calcul des m√©triques (moyenne sur l'horizon de 10 jours)\n",
    "    rmse = np.mean([np.sqrt(mean_squared_error(y_test_rescaled[:, t], y_pred_rescaled[:, t])) for t in range(prediction_size)])\n",
    "    mae = np.mean([mean_absolute_error(y_test_rescaled[:, t], y_pred_rescaled[:, t]) for t in range(prediction_size)])\n",
    "    r2 = np.mean([r2_score(y_test_rescaled[:, t], y_pred_rescaled[:, t]) for t in range(prediction_size)])\n",
    "\n",
    "    print(f\"üìä R√©sultats pour window_size={window_size} -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R¬≤: {r2:.3f}\")\n",
    "\n",
    "    # üîπ Stocker les r√©sultats\n",
    "    results.append((window_size, rmse, mae, r2))\n",
    "\n",
    "# üîπ Afficher le meilleur r√©sultat\n",
    "best_window = min(results, key=lambda x: x[1])  # Choix bas√© sur le RMSE le plus bas\n",
    "print(f\"\\n‚úÖ Meilleure fen√™tre : {best_window[0]} avec RMSE={best_window[1]:.3f}, MAE={best_window[2]:.3f}, R¬≤={best_window[3]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# üîπ D√©finition de la fonction pour cr√©er des s√©quences\n",
    "def create_sequences(features, target, window_size, prediction_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - window_size - prediction_size):\n",
    "        X.append(features[i:i+window_size])\n",
    "        y.append(target[i+window_size:i+window_size+prediction_size])\n",
    "    return torch.FloatTensor(X).to(device), torch.FloatTensor(y).to(device)\n",
    "\n",
    "# üîπ Listes des tailles de fen√™tres √† tester\n",
    "window_sizes = [10,]#20,30, 60, 90, 120]\n",
    "prediction_size = 10  # Fixe (peut √™tre ajust√©)\n",
    "batch_size = 48  # D√©finition de la taille des batchs\n",
    "results = []\n",
    "models = []\n",
    "\n",
    "# üîπ Boucle sur diff√©rentes tailles de fen√™tres\n",
    "for window_size in window_sizes:\n",
    "    print(f\"\\nüü¢ Test avec window_size = {window_size}\")\n",
    "\n",
    "    # Cr√©ation des s√©quences\n",
    "    X_train, y_train = create_sequences(train_features, train_target, window_size, prediction_size)\n",
    "    X_val, y_val = create_sequences(val_features, val_target, window_size, prediction_size)\n",
    "    X_test, y_test = create_sequences(test_features, test_target, window_size, prediction_size)\n",
    "\n",
    "    # Cr√©ation des DataLoaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
    "\n",
    "    # üîπ V√©rification des dimensions\n",
    "    model = LSTMModel(X_train.shape[2], window_size, prediction_size).to(device)\n",
    "    learner = Learner(model, train_loader, val_loader)\n",
    "    learner.fit(lr=0.0001, epochs=20)\n",
    "\n",
    "    y_pred = learner.predict(test_loader)\n",
    "\n",
    "    # üîπ Inversion de l'√©chelle si n√©cessaire\n",
    "    y_test_rescaled = scaler_target.inverse_transform(y_test.reshape(-1, prediction_size))\n",
    "    y_pred_rescaled = scaler_target.inverse_transform(y_pred.reshape(-1, prediction_size))\n",
    "\n",
    "    # üîπ Calcul des m√©triques (moyenne sur l'horizon de 10 jours)\n",
    "    rmse = np.mean([np.sqrt(mean_squared_error(y_test_rescaled[:, t], y_pred_rescaled[:, t])) for t in range(prediction_size)])\n",
    "    mae = np.mean([mean_absolute_error(y_test_rescaled[:, t], y_pred_rescaled[:, t]) for t in range(prediction_size)])\n",
    "    r2 = np.mean([r2_score(y_test_rescaled[:, t], y_pred_rescaled[:, t]) for t in range(prediction_size)])\n",
    "\n",
    "    print(f\"üìä R√©sultats pour window_size={window_size} -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R¬≤: {r2:.3f}\")\n",
    "\n",
    "    # üîπ Stocker les r√©sultats\n",
    "    results.append((window_size, rmse, mae, r2))\n",
    "\n",
    "# üîπ Afficher le meilleur r√©sultat\n",
    "best_window = min(results, key=lambda x: x[1])  # Choix bas√© sur le RMSE le plus bas\n",
    "print(f\"\\n‚úÖ Meilleure fen√™tre : {best_window[0]} avec RMSE={best_window[1]:.3f}, MAE={best_window[2]:.3f}, R¬≤={best_window[3]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Assesment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraire les valeurs\n",
    "window_sizes, rmse_values, mae_values, r2_values = zip(*results)\n",
    "\n",
    "# Tracer l'√©volution du RMSE\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(window_sizes, rmse_values, marker='o', linestyle='-', color='b', label=\"RMSE\")\n",
    "plt.xlabel(\"Taille de la fen√™tre temporelle\")\n",
    "plt.ylabel(\"Erreur (RMSE)\")\n",
    "plt.title(\"Impact de la fen√™tre temporelle sur la pr√©cision\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "* Si window_size est trop petit, le mod√®le manque de contexte et ne capture pas bien les tendances.\n",
    "* Si window_size est trop grand, il risque d‚Äôavoir trop d‚Äôinformations inutiles et de perdre en g√©n√©ralisation.\n",
    "* Le test permet de trouver un compromis optimal pour minimiser lerreur (RMSE, MAE) et maximiser R¬≤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# Encha√Æner avec la meilleure fen√™tre temporelle\n",
    "* Apr√®s avoir trouv√© la meilleure taille de fen√™tre temporelle (best_window), l'id√©e est de :\n",
    "\n",
    "* R√©entra√Æner le mod√®le LSTM avec cette meilleure window_size.\n",
    "* Effectuer la pr√©diction finale sur X_test.\n",
    "* Analyser l'erreur moyenne par jour de pr√©diction pour voir comment elle √©volue sur les 10 jours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# üîπ D√©finition de la meilleure fen√™tre temporelle (apr√®s test)\n",
    "best_window_size = 10 #best_window[0]  # R√©cup√©rer la taille optimale trouv√©e pr√©c√©demment\n",
    "prediction_size = 10  # Horizon de pr√©vision\n",
    "\n",
    "# üîπ Cr√©ation des s√©quences avec la meilleure fen√™tre\n",
    "X_train, y_train = create_sequences(train_features, train_target, best_window_size, prediction_size)\n",
    "X_val, y_val = create_sequences(val_features, val_target, best_window_size, prediction_size)\n",
    "X_test, y_test = create_sequences(test_features, test_target, best_window_size, prediction_size)\n",
    "x_test, y_test_mgb = create_sequences(test_features, test_targets, best_window_size, prediction_size)\n",
    "\n",
    "# üîπ V√©rification des dimensions\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "\n",
    "# üîπ Red√©finition du mod√®le LSTM\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(256, return_sequences=True, input_shape=(best_window_size, X_train.shape[2]))),\n",
    "    Dropout(0.2),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(prediction_size)\n",
    "])\n",
    "\n",
    "# üîπ Compilation et entra√Ænement\n",
    "model.compile(optimizer='adam',loss=Huber(), metrics=['mae'])\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# üîπ Pr√©diction finale avec la meilleure fen√™tre\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# üîπ Inversion de l'√©chelle des pr√©dictions et des valeurs r√©elles\n",
    "y_test_rescaled = scaler_target.inverse_transform(y_test.reshape(-1, prediction_size))\n",
    "y_pred_rescaled = scaler_target.inverse_transform(y_pred.reshape(-1, prediction_size))\n",
    "y_test_mgb_rescaled = scaler_target.inverse_transform(y_test_mgb.reshape(-1, prediction_size))\n",
    "\n",
    "# üîπ Calcul des m√©triques de performance\n",
    "rmse = np.mean([np.sqrt(mean_squared_error(y_test_rescaled[:, t], y_pred_rescaled[:, t])) for t in range(prediction_size)])\n",
    "mae = np.mean([mean_absolute_error(y_test_rescaled[:, t], y_pred_rescaled[:, t]) for t in range(prediction_size)])\n",
    "r2 = np.mean([r2_score(y_test_rescaled[:, t], y_pred_rescaled[:, t]) for t in range(prediction_size)])\n",
    "\n",
    "print(f\"\\nüìä R√©sultats finaux avec window_size={best_window_size} -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R¬≤: {r2:.3f}\")\n",
    "\n",
    "# üîπ Calcul de l'erreur absolue par jour de pr√©diction\n",
    "error = np.abs(y_test_rescaled - y_pred_rescaled)\n",
    "\n",
    "# üîπ Affichage de l'√©volution de l'erreur moyenne par jour de pr√©diction\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(np.mean(error, axis=0), marker='o', linestyle='dashed', color='red')\n",
    "\n",
    "plt.xlabel(\"Jour de pr√©diction de (t+1 √† t+10)\")\n",
    "plt.ylabel(\"Erreur moyenne\")\n",
    "plt.title(\"√âvolution de l'erreur de pr√©diction par jour\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# avec loss =Huber\n",
    "# üîπ Reconstruction des s√©ries temporelles\n",
    "y_test_mgb_reconstructed = np.concatenate([y_test_mgb_rescaled[i] for i in range(y_test_mgb_rescaled.shape[0] - 1)], axis=0)\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# üîπ Cr√©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de pr√©diction\n",
    "    # S√©lectionner les valeurs correspondantes aux s√©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les pr√©dictions pour le jour i\n",
    "    y_test_mgb =y_test_mgb_reconstructed [i::prediction_size]  # Prend les pr√©dictions pour le jour i\n",
    "    # Calcul des m√©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corr√©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Q_obs\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Q_pred (pr√©dit)\", color='red', linestyle='solid')\n",
    "    axes[i].plot(y_test_mgb, label=\"Q_mgb\", color='black')\n",
    "\n",
    "    # Ajouter la m√©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs r√©elles et pr√©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# üîπ Reconstruction des cibles et pr√©dictions\n",
    "y_test_reconstructed = np.concatenate(y_test_rescaled[:-1], axis=0)\n",
    "y_pred_reconstructed = np.concatenate(y_pred_rescaled[:-1], axis=0)\n",
    "\n",
    "# üîπ R√©cup√©ration du d√©bit_mgb dans les features (par exemple √† l‚Äôindice 1)\n",
    "debit_mgb_index = 1\n",
    "\n",
    "# üîπ On r√©cup√®re la derni√®re valeur de chaque s√©quence (align√©e avec chaque pr√©diction de 10 jours)\n",
    "debit_mgb_series = X_test[:-1, -1, debit_mgb_index].reshape(-1, 1)\n",
    "\n",
    "# üîπ R√©p√©ter cette derni√®re valeur pour chaque jour d‚Äôhorizon\n",
    "# Car chaque s√©quence g√©n√®re 10 pr√©dictions (horizon = 10)\n",
    "debit_mgb_reconstructed = np.tile(debit_mgb_series, (1, prediction_size)).reshape(-1)\n",
    "\n",
    "# üîπ Cr√©ation des sous-graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):\n",
    "    y_true = y_test_reconstructed[i::prediction_size]\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]\n",
    "    debit_mgb = debit_mgb_reconstructed[i::prediction_size]\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    corr = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "\n",
    "    axes[i].plot(y_true, label=\"Y_test (r√©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred\", color='red', linestyle='solid')\n",
    "    axes[i].plot(debit_mgb, label=\"D√©bit MGB\", color='green', linestyle='dotted')\n",
    "\n",
    "    axes[i].set_title(f\"Jour {i+1} ‚Äì RMSE={rmse:.2f}, Corr={corr:.2f}\")\n",
    "    axes[i].legend()\n",
    "    axes[i].grid()\n",
    "\n",
    "plt.suptitle(\"Comparaison r√©elle / pr√©dite avec D√©bit MGB sur les 10 jours d‚Äôhorizon\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape de X_test :\", x_rain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : on suppose que la variable pluie est la premi√®re colonne (colonne 0) des features\n",
    "# Donc on r√©cup√®re la m√™me variable dans les X_test (m√™me d√©coupage que y_test)\n",
    "rain_feature = test_features[:, 0]  # ou change le 0 selon la variable souhait√©e\n",
    "\n",
    "# On recr√©e les s√©quences pour matcher les index\n",
    "y_rain = create_sequences(test_features, test_target, window_size=best_window_size, prediction_size=prediction_size)\n",
    "\n",
    "# Comme y_rain n'est pas utilis√© pour pr√©dire mais pour visualiser, on peut extraire la m√™me logique\n",
    "rain_reconstructed = np.concatenate([y_rain[i] for i in range(y_rain.shape[0] - 1)], axis=0)\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (r√©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (pr√©dit)\", color='red', linestyle='solid')\n",
    "    axes[i].plot(rain_reconstructed[i::prediction_size], label=\"Pluie (entr√©e)\", color='green', linestyle='dotted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# avec loss =Huber\n",
    "# üîπ Reconstruction des s√©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# üîπ Cr√©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de pr√©diction\n",
    "    # S√©lectionner les valeurs correspondantes aux s√©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les pr√©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des m√©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corr√©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (r√©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (pr√©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la m√©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs r√©elles et pr√©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# üîπ Reconstruction des s√©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# üîπ Cr√©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de pr√©diction\n",
    "    # S√©lectionner les valeurs correspondantes aux s√©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les pr√©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des m√©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corr√©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (r√©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (pr√©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la m√©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs r√©elles et pr√©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# üîπ Reconstruction des s√©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# üîπ Cr√©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de pr√©diction\n",
    "    # S√©lectionner les valeurs correspondantes aux s√©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les pr√©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des m√©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corr√©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (r√©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (pr√©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la m√©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs r√©elles et pr√©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# üîπ Reconstruction des s√©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# üîπ Cr√©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de pr√©diction\n",
    "    # S√©lectionner les valeurs correspondantes aux s√©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les pr√©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des m√©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corr√©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (r√©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (pr√©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la m√©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs r√©elles et pr√©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# üîπ Reconstruction des s√©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# üîπ Cr√©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de pr√©diction\n",
    "    # S√©lectionner les valeurs correspondantes aux s√©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les pr√©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des m√©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corr√©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (r√©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (pr√©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la m√©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs r√©elles et pr√©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# üîπ Reconstruction des s√©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# üîπ Cr√©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de pr√©diction\n",
    "    # S√©lectionner les valeurs correspondantes aux s√©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les pr√©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des m√©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corr√©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (r√©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (pr√©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la m√©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs r√©elles et pr√©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# üîπ Reconstruction des s√©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# üîπ Cr√©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de pr√©diction\n",
    "    # S√©lectionner les valeurs correspondantes aux s√©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les pr√©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des m√©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corr√©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (r√©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (pr√©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la m√©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs r√©elles et pr√©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# üîπ Reconstruction des s√©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# üîπ Cr√©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de pr√©diction\n",
    "    # S√©lectionner les valeurs correspondantes aux s√©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les pr√©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des m√©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corr√©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (r√©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (pr√©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la m√©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs r√©elles et pr√©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# üîπ Reconstruction des s√©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# üîπ Cr√©ation des sous-graphiques avec Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de pr√©diction\n",
    "    # S√©lectionner les valeurs correspondantes aux s√©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les pr√©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des m√©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corr√©lation\n",
    "    \n",
    "    # Ajouter un sous-graphique pour chaque jour\n",
    "    fig.add_trace(go.Scatter(y=y_true, mode='lines', name=f'Y_test Jour {i+1}', line=dict(color='blue')))\n",
    "    fig.add_trace(go.Scatter(y=y_pred, mode='lines', name=f'Y_pred Jour {i+1}', line=dict(color='red', dash='dash')))\n",
    "    \n",
    "    # Ajouter des annotations\n",
    "    fig.add_annotation(\n",
    "        x=len(y_true) - 1, y=max(y_true),\n",
    "        text=f\"Jour {i+1}<br>RMSE={rmse:.2f}, Corr={correlation:.2f}\",\n",
    "        showarrow=False, font=dict(size=10)\n",
    "    )\n",
    "\n",
    "# Mise en forme du graphe\n",
    "fig.update_layout(\n",
    "    title=\"Comparaison des valeurs r√©elles et pr√©dites pour chaque jour de l'horizon de 10 jours\",\n",
    "    xaxis_title=\"Temps\",\n",
    "    yaxis_title=\"Valeurs\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Afficher le graphique\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def kge(sim, obs):\n",
    "    cc = np.corrcoef(sim, obs)[0, 1]\n",
    "    alpha = np.std(sim) / np.std(obs)\n",
    "    beta = np.mean(sim) / np.mean(obs)\n",
    "    return 1 - np.sqrt((cc - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "\n",
    "def pbias(sim, obs):\n",
    "    return 100 * np.sum(sim - obs) / np.sum(obs)\n",
    "\n",
    "# üîπ Reconstruction des s√©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# üîπ Cr√©ation du subplot interactif\n",
    "fig = make_subplots(rows=5, cols=2, subplot_titles=[f\"Jour {i+1}\" for i in range(10)])\n",
    "\n",
    "for i in range(10):  # Pour chaque jour de l'horizon de pr√©diction\n",
    "    y_true = y_test_reconstructed[i::10]\n",
    "    y_pred = y_pred_reconstructed[i::10]\n",
    "    \n",
    "    # Calcul des m√©triques\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    kge_value = kge(y_pred, y_true)\n",
    "    pbias_value = pbias(y_pred, y_true)\n",
    "    \n",
    "    # Ajouter la courbe des valeurs r√©elles\n",
    "    fig.add_trace(go.Scatter(y=y_true, mode='lines', name='Y_test (r√©el)', line=dict(color='blue')),\n",
    "                  row=(i//2)+1, col=(i%2)+1)\n",
    "    \n",
    "    # Ajouter la courbe des valeurs pr√©dites\n",
    "    fig.add_trace(go.Scatter(y=y_pred, mode='lines', name='Y_pred (pr√©dit)', line=dict(color='red', dash='dash')),\n",
    "                  row=(i//2)+1, col=(i%2)+1)\n",
    "    \n",
    "    # Ajouter les m√©triques dans le titre du sous-graphe\n",
    "    fig.update_annotations(\n",
    "        selector=dict(text=f\"Jour {i+1}\"),\n",
    "        text=f\"Jour {i+1} \\nRMSE={rmse:.2f}, Corr={correlation:.2f}, KGE={kge_value:.2f}, PBIAS={pbias_value:.2f}%\"\n",
    "    )\n",
    "\n",
    "# Configuration de la mise en page\n",
    "title_text = \"Comparaison des valeurs r√©elles et pr√©dites pour chaque jour de l'horizon de 10 jours\"\n",
    "fig.update_layout(height=900, width=1200, title_text=title_text, showlegend=False)\n",
    "\n",
    "# Affichage interactif\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# üîπ Fonction pour calculer le Kling-Gupta Efficiency (KGE)\n",
    "def kling_gupta_efficiency(y_true, y_pred):\n",
    "    mean_obs = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    \n",
    "    r = np.corrcoef(y_true, y_pred)[0, 1]  # Corr√©lation\n",
    "    beta = mean_pred / mean_obs  # Biais moyen\n",
    "    alpha = (np.std(y_pred) / mean_pred) / (np.std(y_true) / mean_obs)  # Ratio des √©carts-types\n",
    "    \n",
    "    return 1 - np.sqrt((r - 1) ** 2 + (beta - 1) ** 2 + (alpha - 1) ** 2)\n",
    "\n",
    "# üîπ Fonction pour calculer le PBIAS\n",
    "def pbias(y_true, y_pred):\n",
    "    return 100 * np.sum(y_pred - y_true) / np.sum(y_true)\n",
    "\n",
    "# üîπ Reconstruction des s√©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# üîπ Cr√©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de pr√©diction\n",
    "    # S√©lectionner les valeurs correspondantes aux s√©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les pr√©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des m√©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corr√©lation\n",
    "    kge = kling_gupta_efficiency(y_true, y_pred)  # KGE\n",
    "    pbias_value = pbias(y_true, y_pred)  # PBIAS\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (r√©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (pr√©dit)\", color='red', linestyle='dashed')\n",
    "\n",
    "    # Ajouter la m√©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}, KGE={kge:.2f}, PBIAS={pbias_value:.2f}%\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs r√©elles et pr√©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# üîπ Chemin du r√©pertoire contenant les fichiers .txt\n",
    "\n",
    "repertoire =\"/home/mgning/work/hyfaa/work_configurations/senegal/test_oualia_cs=1_wm=800_b=0.30/\"\n",
    "# üîπ Liste des fichiers .txt dans le r√©pertoire\n",
    "fichiers = [f for f in os.listdir(repertoire) if f.endswith('.txt')]\n",
    "# üîπ Cr√©ation d'un DataFrame vide pour fusionner les donn√©es\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "# üîπ Boucle sur tous les fichiers .txt\n",
    "for fichier in fichiers:\n",
    "    chemin_fichier = os.path.join(repertoire, fichier)\n",
    "    \n",
    "    # Lire le fichier en tant que DataFrame\n",
    "    df = pd.read_csv(chemin_fichier, sep=\";\")  # Modifier 'sep' si n√©cessaire\n",
    "    \n",
    "    # V√©rifier si la colonne \"value\" existe\n",
    "    if \"value\" in df.columns:\n",
    "        # Renommer la colonne \"value\" avec le nom du fichier (sans extension)\n",
    "        nom_colonne = os.path.splitext(fichier)[0]\n",
    "        df = df[[\"value\"]].rename(columns={\"value\": nom_colonne})\n",
    "        \n",
    "        # Fusionner les DataFrames (concat√©nation horizontale)\n",
    "        if df_final.empty:\n",
    "            df_final = df\n",
    "        else:\n",
    "            df_final = pd.concat([df_final, df], axis=1)\n",
    "\n",
    "# üîπ Affichage du DataFrame final\n",
    "print(df_final.head())\n",
    "\n",
    "# üîπ Sauvegarde en CSV si besoin\n",
    "df_final.to_csv(\"resultat_final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Charger les donn√©es\n",
    "df = df_final.copy()  # Assurez-vous que df_final contient les bonnes donn√©es\n",
    "\n",
    "# S√©lection des colonnes\n",
    "features_columns = [\"Q_MANANTALI_AVAL\", \"Q_GUIERS\", \"Q_GHORFA\", \"Q_KABATE\", \"Q_BAFING_MAKANA\"]\n",
    "targets_columns = [\"Q_MANANTALI_AVAL\", \"Q_GUIERS\", \"Q_GHORFA\", \"Q_KABATE\", \"Q_BAFING_MAKANA\"]\n",
    "\n",
    "# Normalisation des donn√©es\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df[features_columns]), columns=features_columns, index=df.index)\n",
    "\n",
    "# Fonction pour cr√©er les s√©quences de donn√©es\n",
    "def create_sequences(data, target_columns, window_size, prediction_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - prediction_size):\n",
    "        X.append(data.iloc[i:i+window_size].values)\n",
    "        y.append(data.iloc[i+window_size:i+window_size+prediction_size][target_columns].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Param√®tres\n",
    "window_size = 10  # Nombre de jours utilis√©s pour la pr√©diction\n",
    "prediction_size = 10  # Nombre de jours √† pr√©dire\n",
    "\n",
    "# Cr√©ation des jeux d'entra√Ænement et de test\n",
    "train_size = int(len(df_scaled) * 0.3)\n",
    "train_data = df_scaled.iloc[:train_size]\n",
    "test_data = df_scaled.iloc[train_size:]\n",
    "\n",
    "X_train, y_train = create_sequences(train_data, targets_columns, window_size, prediction_size)\n",
    "X_test, y_test = create_sequences(test_data, targets_columns, window_size, prediction_size)\n",
    "\n",
    "# Construction du mod√®le LSTM\n",
    "model = Sequential([\n",
    "    LSTM(100, activation='relu', return_sequences=True, input_shape=(window_size, len(features_columns))),\n",
    "    LSTM(50, activation='relu'),\n",
    "    Dense(len(targets_columns) * prediction_size)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Entra√Ænement du mod√®le\n",
    "history = model.fit(X_train, y_train.reshape(y_train.shape[0], -1), epochs=50, batch_size=32, validation_data=(X_test, y_test.reshape(y_test.shape[0], -1)))\n",
    "\n",
    "# Pr√©diction\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.reshape(y_test.shape)  # Reshape pour correspondre aux dimensions originales\n",
    "\n",
    "# Inverser la normalisation\n",
    "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, len(targets_columns))).reshape(y_test.shape)\n",
    "y_pred_rescaled = scaler.inverse_transform(y_pred.reshape(-1, len(targets_columns))).reshape(y_pred.shape)\n",
    "\n",
    "# üìä Visualisation des pr√©dictions vs valeurs r√©elles\n",
    "fig, axes = plt.subplots(5, 1, figsize=(12, 15), sharex=True)\n",
    "\n",
    "for i, col in enumerate(targets_columns):\n",
    "    axes[i].plot(y_test_rescaled[:, :, i].flatten(), label=\"R√©el\", color='blue')\n",
    "    axes[i].plot(y_pred_rescaled[:, :, i].flatten(), label=\"Pr√©dit\", color='red', linestyle='dashed')\n",
    "    axes[i].set_title(f\"Pr√©diction de {col}\")\n",
    "    axes[i].legend()\n",
    "    axes[i].grid()\n",
    "\n",
    "plt.xlabel(\"Temps\")\n",
    "plt.suptitle(\"Comparaison des valeurs r√©elles et pr√©dites par LSTM\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Learning rate finder development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_find(self, start_lr=1e-7, end_lr=10, num_iter=100, step_mode=\"exp\", show_plot=True):\n",
    "        \"\"\"Find a good learning rate by training with exponentially growing lr\n",
    "            source: https://github.com/fastai/fastai1/blob/master/fastai/train.py#L33\n",
    "\n",
    "        \n",
    "        Args:\n",
    "            start_lr (float): Starting learning rate\n",
    "            end_lr (float): Maximum learning rate\n",
    "            num_iter (int): Number of iterations to run\n",
    "            step_mode (str): \"exp\" for exponential increase, \"linear\" for linear increase\n",
    "            show_plot (bool): Whether to display the loss plot\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (optimal_lr, learning_rates, losses)\n",
    "        \"\"\"\n",
    "        # Save the original model state\n",
    "        original_state = {\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer\n",
    "        }\n",
    "        \n",
    "        # Initialize optimizer with start_lr\n",
    "        optimizer = self.optimizer(self.model.parameters(), lr=start_lr)\n",
    "        \n",
    "        # Calculate the multiplication factor for each step\n",
    "        if step_mode == \"exp\":\n",
    "            gamma = (end_lr / start_lr) ** (1 / num_iter)\n",
    "        else:\n",
    "            gamma = (end_lr - start_lr) / num_iter\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma) if step_mode == \"exp\" else None\n",
    "        \n",
    "        learning_rates = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        # Create iterator for training data\n",
    "        iterator = iter(self.train_loader)\n",
    "        \n",
    "        for iteration in range(num_iter):\n",
    "            try:\n",
    "                batch_X, batch_y = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(self.train_loader)\n",
    "                batch_X, batch_y = next(iterator)\n",
    "                \n",
    "            # Forward pass\n",
    "            self.model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(batch_X)\n",
    "            loss = self.criterion(outputs, batch_y.squeeze())\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Store the values\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            learning_rates.append(current_lr)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Update learning rate\n",
    "            if step_mode == \"exp\":\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = start_lr + (gamma * (iteration + 1))\n",
    "            \n",
    "            # Stop if the loss is exploding\n",
    "            if iteration > 0 and losses[-1] > 4 * best_loss:\n",
    "                break\n",
    "                \n",
    "            if losses[-1] < best_loss:\n",
    "                best_loss = losses[-1]\n",
    "        \n",
    "        # Restore the original model state\n",
    "        self.model.load_state_dict(original_state['model'])\n",
    "        \n",
    "        if show_plot:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(learning_rates, losses)\n",
    "            plt.xscale('log')\n",
    "            plt.xlabel('Learning Rate (log scale)')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Learning Rate Finder')\n",
    "            plt.show()\n",
    "            \n",
    "        # Find the point of steepest descent\n",
    "        smoothed_losses = np.array(losses)\n",
    "        min_grad_idx = np.gradient(smoothed_losses).argmin()\n",
    "        optimal_lr = learning_rates[min_grad_idx]\n",
    "            \n",
    "        return optimal_lr, learning_rates, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## Initial Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"delta_debit_mgb\"] = data[\"d√©bit_mgb\"].diff().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cr√©ation de la figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Ajout de la courbe pour P_cumul_7j (Axe principal Y1)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data.index, \n",
    "    y=data['d√©bit_insitu'], \n",
    "    mode='lines', \n",
    "    name=\"Pluie cumul√©e 7j\", \n",
    "    line=dict(color='blue'),\n",
    "    yaxis=\"y1\"  # Sp√©cifier l'axe Y1\n",
    "))\n",
    "\n",
    "# Ajout de la courbe pour d√©bit_insitu (Axe secondaire Y2)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data.index, \n",
    "    y=data[\"d√©bit_mgb\"], \n",
    "    mode='lines', \n",
    "    name=\"D√©bit in situ\", \n",
    "    line=dict(color='red', dash='dash'),\n",
    "    yaxis=\"y2\"  # Sp√©cifier l'axe Y2\n",
    "))\n",
    "\n",
    "# Mise en forme du graphique avec deux axes Y\n",
    "fig.update_layout(\n",
    "    title=\"√âvolution de P_cumul_7j et D√©bit in situ en fonction du temps\",\n",
    "    xaxis=dict(title=\"Temps\"),\n",
    "    yaxis=dict(\n",
    "        title=\"Pluie cumul√©e 7j (mm)\", \n",
    "        #titlefont=dict(color=\"blue\"), \n",
    "        tickfont=dict(color=\"blue\"),\n",
    "        side=\"left\"\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title=\"D√©bit in situ (m¬≥/s)\", \n",
    "        #titlefont=dict(color=\"red\"), \n",
    "        tickfont=dict(color=\"red\"),\n",
    "        overlaying=\"y\",  # Superposition sur l'axe principal\n",
    "        side=\"right\"  # Placement √† droite\n",
    "    ),\n",
    "    legend=dict(x=0.02, y=0.98),  # Position de la l√©gende\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Affichage\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Cr√©ation de la figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Ajout de la courbe pour P_cumul_7j (Axe principal Y1)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data.index, \n",
    "    y=data['d√©bit_insitu'], \n",
    "    mode='lines', \n",
    "    name=\"Pluie cumul√©e 7j\", \n",
    "    line=dict(color='blue'),\n",
    "    yaxis=\"y1\"  # Sp√©cifier l'axe Y1\n",
    "))\n",
    "\n",
    "# Ajout de la courbe pour d√©bit_insitu (Axe secondaire Y2)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data.index, \n",
    "    y=data[\"delta_debit_mgb\"], \n",
    "    mode='lines', \n",
    "    name=\"D√©bit in situ\", \n",
    "    line=dict(color='red', dash='dash'),\n",
    "    yaxis=\"y2\"  # Sp√©cifier l'axe Y2\n",
    "))\n",
    "\n",
    "# Mise en forme du graphique avec deux axes Y\n",
    "fig.update_layout(\n",
    "    title=\"√âvolution de P_cumul_7j et D√©bit in situ en fonction du temps\",\n",
    "    xaxis=dict(title=\"Temps\"),\n",
    "    \n",
    "    yaxis=dict(\n",
    "        title=\"Pluie cumul√©e 7j (mm)\", \n",
    "        # titlefont=dict(color=\"blue\"), \n",
    "        tickfont=dict(color=\"blue\"),\n",
    "        side=\"left\"\n",
    "    ),\n",
    "    \n",
    "    yaxis2=dict(\n",
    "        title=\"D√©bit in situ (m¬≥/s)\", \n",
    "        # titlefont=dict(color=\"red\"), \n",
    "        tickfont=dict(color=\"red\"),\n",
    "        overlaying=\"y\",  # Superposition sur l'axe principal\n",
    "        side=\"right\"  # Placement √† droite\n",
    "    ),\n",
    "\n",
    "    legend=dict(x=0.02, y=0.98),  # Position de la l√©gende\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Affichage\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
