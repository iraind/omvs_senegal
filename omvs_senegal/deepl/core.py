# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/04_deepl.core.ipynb.

# %% auto 0
__all__ = ['split_by_date', 'HydroDataset', 'Learner']

# %% ../../nbs/04_deepl.core.ipynb 2
import numpy as np
import pandas as pd
from tqdm import tqdm
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter

# %% ../../nbs/04_deepl.core.ipynb 8
def split_by_date(
        data: pd.DataFrame, # Input dataframe containing time series data
        val_dates: tuple,   # Tuple of (start_date, end_date) for validation set
        test_dates: tuple   # Tuple of (start_date, end_date) for test set
        ) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """Split time series data into train, validation and test sets based on date ranges."""
    val_data = data[val_dates[0]:val_dates[1]]
    test_data = data[test_dates[0]:test_dates[1]]
    train_data = data[~(data.index.isin(val_data.index) | data.index.isin(test_data.index))]
    print(f"Approximate data repartition:\n"
          f"Train: {train_data.shape[0]/data.shape[0]:.2%}\n"
          f"Validation: {val_data.shape[0]/data.shape[0]:.2%}\n" 
          f"Test: {test_data.shape[0]/data.shape[0]:.2%}")
    return train_data, val_data, test_data

# %% ../../nbs/04_deepl.core.ipynb 16
class HydroDataset(Dataset):
    def __init__(
            self,
            x: pd.DataFrame,
            y: pd.DataFrame, 
            ctx_len: int, 
            pred_len: int = 10, 
            x_transform: callable = None,
            y_transform: callable = None):
        
        if x_transform is None:
            self.features = x.copy()
        else:
            self.features = pd.DataFrame(x_transform(x), columns=x.columns, index=x.index)
        if y_transform is None:
            self.targets = y.copy()
        else:
            self.targets = pd.DataFrame(y_transform(y), columns=y.columns, index=y.index)
        
        self.context_length = ctx_len
        self.prediction_length = pred_len
        self.x_transform = x_transform
        self.y_transform = y_transform
        
    def __len__(self):
        return self.features.shape[0] - self.context_length - self.prediction_length + 1
    
    def __getitem__(self, idx):
        # Get sequence of features
        features = self.features[idx:idx + self.context_length]
        # Get target (next value after sequence)
        targets = self.targets[idx + self.context_length:idx + self.context_length + self.prediction_length]
        return torch.FloatTensor(features.values), torch.FloatTensor(targets.values)
    
    def get_t0(self, idx):
        """Get the t+0 in the sequence from where forecast is made."""
        return self.features.index[idx + self.context_length-1]

# %% ../../nbs/04_deepl.core.ipynb 28
class Learner:
    def __init__(self,
                 model: nn.Module, # model to train
                 train_loader: DataLoader, # data loader for training data
                 val_loader: DataLoader, # data loader for validation data
                 criterion: nn.Module = nn.MSELoss(), # loss function to optimize
                 optimizer: torch.optim.Optimizer = torch.optim.Adam, # optimizer class to use for training
                 log_dir: str = None, # directory to save tensorboard logs,
                 verbose: bool = True # whether to print training progress
                 ) -> None:
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.criterion = criterion
        self.optimizer = optimizer
        self.writer = None if log_dir is None else SummaryWriter(log_dir)
        self.verbose = verbose

    def fit(self, lr=0.001, epochs=10):
        optimizer = self.optimizer(self.model.parameters(), lr=lr)
        for epoch in tqdm(range(epochs), desc='Training epochs'):
            # Training
            self.model.train()
            epoch_loss = 0
            for batch_X, batch_y in self.train_loader:
                optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = self.criterion(outputs, batch_y.squeeze())
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()

            avg_train_loss = epoch_loss/len(self.train_loader)
            if self.writer is not None:
                self.writer.add_scalar('Training Loss/epoch', avg_train_loss, epoch)

            # Validation
            self.model.eval()
            val_loss = 0
            with torch.no_grad():
                for batch_X, batch_y in self.val_loader:
                    val_outputs = self.model(batch_X)
                    val_loss += self.criterion(val_outputs, batch_y.squeeze()).item()
            
            avg_val_loss = val_loss/len(self.val_loader)
            if self.writer is not None:
                self.writer.add_scalar('Validation Loss/epoch', avg_val_loss, epoch)

            if self.verbose:
                print(f'Epoch {epoch+1}, Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')
    
    def predict_values(self, dl: DataLoader):
        self.model.eval()
        predictions = []
        targets = []
        with torch.no_grad():
            for batch_X, batch_y in dl:
                batch_pred = self.model(batch_X).cpu().numpy()
                predictions.append(batch_pred)

        predictions = np.vstack(predictions)
        return predictions
    
    def predict(self, dl: DataLoader, inverse_transform: callable=None) -> pd.DataFrame:
        """Make predictions and return them as a pandas DataFrame with proper indexing and column names."""

        predictions = self.predict_values(dl)
        
        # Get the indices from the dataset
        indices = []
        for batch_X, _ in dl:
            # Assuming the dataset has get_t0 method to get the forecast start time
            if hasattr(dl.dataset, 'get_t0'):
                for i in range(len(batch_X)):
                    idx = len(indices)
                    indices.append(dl.dataset.get_t0(idx))
        
        if inverse_transform is None:
            predictions = predictions.reshape(-1, predictions.shape[-1])
        else:
            predictions = inverse_transform(predictions.reshape(-1, predictions.shape[-1]))
        
        n_horizons = predictions.shape[1] if len(predictions.shape) > 1 else 1
        column_names = [f"t+{i+1}" for i in range(0, n_horizons)]
        
        predictions_df = pd.DataFrame(predictions, index=indices, columns=column_names)
        
        return predictions_df
